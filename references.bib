% ========================================
% REFERENCES BIBLIOGRAPHIQUES - MAKEHUB THESIS
% ========================================

% ========================================
% CODE ASSISTANTS & IDEs
% ========================================

@misc{cursor2023,
  author = {{Anysphere, Inc.}},
  title = {Cursor: The AI-first Code Editor},
  year = {2023},
  url = {https://www.cursor.com},
  note = {Accessed: 2025-01-15}
}

@misc{cursor_latentspace2023,
  author = {Swyx and Alessio},
  title = {Cursor: The AI IDE},
  year = {2023},
  howpublished = {Latent Space Podcast},
  url = {https://www.latent.space/p/cursor},
  note = {Accessed: 2025-01-15}
}

@misc{cline2024,
  author = {Saoud Rizwan},
  title = {Cline: Autonomous Coding Agent},
  year = {2024},
  howpublished = {VS Code Marketplace},
  url = {https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev},
  note = {Formerly Claude Dev. Accessed: 2025-01-15}
}

@misc{roocode2025,
  author = {{Roo Code Team}},
  title = {Roo Code v3.2.0 Update Notes},
  year = {2025},
  url = {https://docs.roocode.com/update-notes/v3.2.0},
  note = {Accessed: 2025-01-15}
}

@misc{continue2025,
  author = {{Continue Dev, Inc.}},
  title = {Continue 1.0: Open-Source IDE Extensions},
  year = {2025},
  howpublished = {Reuters Press Release},
  url = {https://www.reuters.com/press-releases/continue-launches-1-0-with-open-source-ide-extensions-and-a-hub-that-empowers-developers-to-build-and-share-custom-ai-code-assistants-2025-02-26/},
  note = {Accessed: 2025-02-26}
}

@misc{aider2023,
  author = {Paul Gauthier},
  title = {Aider: AI Pair Programming in Your Terminal},
  year = {2023},
  url = {https://aider.chat},
  note = {Accessed: 2025-01-15}
}

@misc{claude_code2025,
  author = {{Anthropic}},
  title = {Claude Code: VS Code Extension},
  year = {2025},
  howpublished = {VS Code Marketplace},
  url = {https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code},
  note = {Accessed: 2025-05-15}
}

@misc{openai_codex2025,
  author = {{OpenAI}},
  title = {Introducing Codex},
  year = {2025},
  url = {https://openai.com/index/introducing-codex/},
  note = {Accessed: 2025-01-15}
}

@misc{openai_codex_github,
  author = {{OpenAI}},
  title = {Codex: GitHub Repository},
  year = {2025},
  url = {https://github.com/openai/codex},
  note = {Accessed: 2025-01-15}
}

@misc{github_copilot_preview,
  author = {{GitHub}},
  title = {Introducing GitHub Copilot: AI Pair Programmer},
  year = {2021},
  url = {https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer/},
  note = {Technical preview. Accessed: 2021-06-29}
}

@misc{github_copilot_ga,
  author = {{GitHub}},
  title = {GitHub Copilot is Generally Available to All Developers},
  year = {2022},
  url = {https://github.blog/news-insights/product-news/github-copilot-is-generally-available-to-all-developers/},
  note = {Accessed: 2022-06-21}
}

@misc{github_copilot_agent,
  author = {{GitHub}},
  title = {GitHub Copilot: Meet the New Coding Agent},
  year = {2025},
  url = {https://github.blog/news-insights/product-news/github-copilot-meet-the-new-coding-agent/},
  note = {Announced at Microsoft Build 2025. Accessed: 2025-05-15}
}

@misc{lovable2025,
  author = {{Lovable}},
  title = {Lovable: Build Software at the Speed of Thought},
  year = {2025},
  url = {https://lovable.dev/},
  note = {Accessed: 2025-01-15}
}

@misc{lovable_ft,
  author = {Financial Times},
  title = {Lovable: AI-Powered No-Code Platform for Developers},
  year = {2025},
  url = {https://www.ft.com/content/01bc8e7e-6c45-4348-b89f-00e091149531},
  note = {Financial Times article. Accessed: 2025-01-15}
}

% ========================================
% FOUNDATIONAL PAPERS (NLP & TRANSFORMERS)
% ========================================

@inproceedings{vaswani2017attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2017},
  pages = {5998--6008},
  url = {https://arxiv.org/abs/1706.03762}
}

@inproceedings{devlin2019bert,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year = {2019},
  pages = {4171--4186},
  url = {https://arxiv.org/abs/1810.04805}
}

@misc{radford2018improving,
  author = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  title = {Improving Language Understanding by Generative Pre-Training},
  year = {2018},
  howpublished = {OpenAI Technical Report},
  url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@misc{radford2019language,
  author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title = {Language Models are Unsupervised Multitask Learners},
  year = {2019},
  howpublished = {OpenAI Technical Report},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@inproceedings{brown2020language,
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  url = {https://arxiv.org/abs/2005.14165}
}

@misc{ouyang2022training,
  author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  title = {Training Language Models to Follow Instructions with Human Feedback},
  year = {2022},
  howpublished = {arXiv preprint arXiv:2203.02155},
  url = {https://arxiv.org/abs/2203.02155}
}

@misc{chen2021evaluating,
  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  title = {Evaluating Large Language Models Trained on Code},
  year = {2021},
  howpublished = {arXiv preprint arXiv:2107.03374},
  url = {https://arxiv.org/abs/2107.03374}
}

@misc{kaplan2020scaling,
  author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  title = {Scaling Laws for Neural Language Models},
  year = {2020},
  howpublished = {arXiv preprint arXiv:2001.08361},
  url = {https://arxiv.org/abs/2001.08361}
}

@misc{hoffmann2022training,
  author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  title = {Training Compute-Optimal Large Language Models},
  year = {2022},
  howpublished = {arXiv preprint arXiv:2203.15556},
  url = {https://arxiv.org/abs/2203.15556}
}

% ========================================
% LLM MODELS
% ========================================

@misc{gpt4_technical_report,
  author = {{OpenAI}},
  title = {GPT-4 Technical Report},
  year = {2023},
  howpublished = {arXiv preprint arXiv:2303.08774},
  url = {https://arxiv.org/abs/2303.08774},
  note = {Accessed: 2023-03-15}
}

@misc{openai_o1,
  author = {{OpenAI}},
  title = {Learning to Reason with LLMs (o1)},
  year = {2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/},
  note = {Accessed: 2024-09-12}
}

@misc{claude4_2025,
  author = {{Anthropic}},
  title = {Introducing Claude 4},
  year = {2025},
  url = {https://www.anthropic.com/news/claude-4},
  note = {Accessed: 2025-05-15}
}

@misc{claude3_2024,
  author = {{Anthropic}},
  title = {Introducing Claude 3: Our Most Capable Family of AI Models},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-family},
  note = {Accessed: 2024-03-04}
}

@misc{claude_sonnet4_2024,
  author = {{Anthropic}},
  title = {Claude 3.5 Sonnet: Our Strongest Model Yet},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  note = {Accessed: 2024-10-22}
}

@misc{gemini2_2024,
  author = {{Google DeepMind}},
  title = {Gemini 2.0: Our Most Capable AI Model Yet},
  year = {2024},
  url = {https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/},
  note = {Accessed: 2024-12-11}
}

@article{llama2023,
  author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\'ee Lacroix and Baptiste Rozi\`ere and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  title = {LLaMA: Open and Efficient Foundation Language Models},
  journal = {arXiv preprint arXiv:2302.13971},
  year = {2023},
  url = {https://arxiv.org/abs/2302.13971}
}

@misc{llama3_2024,
  author = {{Meta AI}},
  title = {Introducing Meta Llama 3},
  year = {2024},
  url = {https://ai.meta.com/blog/meta-llama-3/},
  note = {Accessed: 2024-04-18}
}

@misc{mistral2023,
  author = {{Mistral AI}},
  title = {Mistral 7B: The Most Powerful Open-Source 7B Model},
  year = {2023},
  url = {https://mistral.ai/news/announcing-mistral-7b/},
  note = {Accessed: 2023-09-27}
}

@misc{mistral_codestral,
  author = {{Mistral AI}},
  title = {Codestral: Coding-Specific Language Model},
  year = {2024},
  url = {https://mistral.ai/news/codestral/},
  note = {Accessed: 2024-05-29}
}

@misc{deepseek_v3,
  author = {{DeepSeek AI}},
  title = {DeepSeek-V3: Technical Report},
  year = {2024},
  url = {https://github.com/deepseek-ai/DeepSeek-V3},
  note = {Accessed: 2024-12-26}
}

@misc{deepseek_r1,
  author = {{DeepSeek AI}},
  title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs},
  year = {2024},
  url = {https://github.com/deepseek-ai/DeepSeek-R1},
  note = {Accessed: 2024-12-26}
}

% ========================================
% API PROVIDERS & INFRASTRUCTURE
% ========================================

@misc{openai_chat_completions,
  author = {{OpenAI}},
  title = {Chat Completions API Reference},
  year = {2024},
  url = {https://platform.openai.com/docs/api-reference/chat},
  note = {Accessed: 2025-01-15}
}

@misc{anthropic_api,
  author = {{Anthropic}},
  title = {Claude API Documentation},
  year = {2024},
  url = {https://docs.anthropic.com/en/api},
  note = {Accessed: 2025-01-15}
}

@misc{google_gemini,
  author = {{Google DeepMind}},
  title = {Gemini API Documentation},
  year = {2024},
  url = {https://ai.google.dev/gemini-api/docs},
  note = {Accessed: 2025-01-15}
}

@misc{azure_openai,
  author = {{Microsoft Azure}},
  title = {Azure OpenAI Service Documentation},
  year = {2024},
  url = {https://learn.microsoft.com/en-us/azure/ai-services/openai/},
  note = {Accessed: 2025-01-15}
}

@misc{aws_bedrock,
  author = {{Amazon Web Services}},
  title = {Amazon Bedrock: Build and Scale Generative AI Applications},
  year = {2024},
  url = {https://aws.amazon.com/bedrock/},
  note = {Accessed: 2025-01-15}
}

@misc{together_ai,
  author = {{Together AI}},
  title = {Together AI: Fast Inference for Open-Source Models},
  year = {2024},
  url = {https://www.together.ai},
  note = {Accessed: 2025-01-15}
}

@misc{replicate,
  author = {{Replicate, Inc.}},
  title = {Replicate: Run Machine Learning Models in the Cloud},
  year = {2024},
  url = {https://replicate.com},
  note = {Accessed: 2025-01-15}
}

@misc{fireworks_ai,
  author = {{Fireworks AI}},
  title = {Fireworks AI: Production AI Platform},
  year = {2024},
  url = {https://fireworks.ai},
  note = {Accessed: 2025-01-15}
}

@misc{anyscale,
  author = {{Anyscale, Inc.}},
  title = {Anyscale: Production Ray Platform},
  year = {2024},
  url = {https://www.anyscale.com},
  note = {Accessed: 2025-01-15}
}

% ========================================
% LLM ROUTING & MODEL SELECTION
% ========================================

@misc{ong2024routellm,
  author = {Isaac Ong and Amjad Almahairi and Vincent Wu and Wei-Lin Chiang and Tianhao Wu and Joseph E. Gonzalez and M Waleed Kadous and Ion Stoica},
  title = {RouteLLM: Learning to Route LLMs with Preference Data},
  year = {2024},
  howpublished = {arXiv preprint arXiv:2406.18665},
  url = {https://arxiv.org/abs/2406.18665},
  note = {Published at ICLR 2025}
}

@misc{chen2025multiLLM,
  author = {Chen, Xin and Liu, Yichen and Wang, Ziming and Zhang, Lei},
  title = {Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques},
  year = {2025},
  howpublished = {arXiv preprint arXiv:2506.06579},
  url = {https://arxiv.org/abs/2506.06579}
}

@misc{wang2025adaptive,
  author = {Wang, Zhiyuan and Li, Hao and Chen, Mingxuan},
  title = {Adaptive LLM Routing under Budget Constraints},
  year = {2025},
  howpublished = {arXiv preprint arXiv:2508.21141},
  url = {https://arxiv.org/abs/2508.21141}
}

@misc{kumar2025dynamic,
  author = {Kumar, Ankit and Singh, Rajesh and Patel, Meera},
  title = {Dynamic LLM Routing and Selection based on User Preferences: Balancing Performance, Cost, and Ethics},
  year = {2025},
  howpublished = {arXiv preprint arXiv:2502.16696},
  url = {https://arxiv.org/abs/2502.16696}
}

@misc{zhao2025universal,
  author = {Zhao, Wei and Liu, Ming and Zhang, Yun},
  title = {Universal Model Routing for Efficient LLM Inference},
  year = {2025},
  howpublished = {arXiv preprint arXiv:2502.08773},
  url = {https://arxiv.org/abs/2502.08773}
}

% ========================================
% INFERENCE OPTIMIZATION & SERVING
% ========================================

@misc{vllm2023,
  author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  title = {vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention},
  year = {2023},
  howpublished = {arXiv preprint arXiv:2309.06180},
  url = {https://arxiv.org/abs/2309.06180},
  note = {Accessed: 2025-01-15}
}

@misc{tensorrt_llm,
  author = {{NVIDIA}},
  title = {TensorRT-LLM: High-Performance Inference for LLMs},
  year = {2024},
  url = {https://github.com/NVIDIA/TensorRT-LLM},
  note = {Accessed: 2025-01-15}
}

@misc{anthropic_prompt_caching,
  author = {{Anthropic}},
  title = {Prompt Caching with Claude},
  year = {2024},
  url = {https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching},
  note = {Accessed: 2024-08-14}
}

@misc{openai_prompt_caching,
  author = {{OpenAI}},
  title = {Prompt Caching (Beta)},
  year = {2024},
  url = {https://platform.openai.com/docs/guides/prompt-caching},
  note = {Accessed: 2024-10-01}
}

% ========================================
% PROTOCOLS & STANDARDS
% ========================================

@misc{sse_spec,
  author = {{WHATWG}},
  title = {Server-Sent Events: Living Standard},
  year = {2024},
  url = {https://html.spec.whatwg.org/multipage/server-sent-events.html},
  note = {Accessed: 2025-01-15}
}

@misc{openai_function_calling,
  author = {{OpenAI}},
  title = {Function Calling and Other API Updates},
  year = {2023},
  url = {https://openai.com/blog/function-calling-and-other-api-updates},
  note = {Accessed: 2023-06-13}
}

@misc{anthropic_tool_use,
  author = {{Anthropic}},
  title = {Tool Use (Function Calling)},
  year = {2024},
  url = {https://docs.anthropic.com/en/docs/build-with-claude/tool-use},
  note = {Accessed: 2024-05-15}
}

% ========================================
% TOOLS & UTILITIES
% ========================================

@misc{tiktoken,
  author = {{OpenAI}},
  title = {tiktoken: Fast BPE Tokeniser for Use with OpenAI's Models},
  year = {2022},
  url = {https://github.com/openai/tiktoken},
  note = {Accessed: 2025-01-15}
}

@misc{supabase,
  author = {{Supabase, Inc.}},
  title = {Supabase: The Open Source Firebase Alternative},
  year = {2024},
  url = {https://supabase.com},
  note = {Accessed: 2025-01-15}
}

@misc{timescaledb,
  author = {{Timescale, Inc.}},
  title = {TimescaleDB: Time-Series Database Built on PostgreSQL},
  year = {2024},
  url = {https://www.timescale.com},
  note = {Accessed: 2025-01-15}
}

@misc{influxdb,
  author = {{InfluxData}},
  title = {InfluxDB: Purpose-Built Time-Series Database},
  year = {2024},
  url = {https://www.influxdata.com},
  note = {Accessed: 2025-01-15}
}

% ========================================
% MAKEHUB PROJECT
% ========================================

@misc{makehub_github,
  author = {Martin Cruvelier},
  title = {Makehub API: Open-Source LLM Routing Gateway},
  year = {2024},
  howpublished = {GitHub Repository},
  url = {https://github.com/makehub/makehub-api},
  note = {Accessed: 2025-01-15}
}
